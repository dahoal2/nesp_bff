{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b642712a-5102-49d9-b2da-e22e6ec4da8a",
   "metadata": {},
   "source": [
    "## Step 1: Extracting variables from ranalysis (BARRA-R2) and projections (BARPA & CSIRO-CCAM) for specific locations\n",
    "Detailed description of BARRA paramters here: https://opus.nci.org.au/spaces/NDP/pages/338002591/BARRA2+Parameter+Descriptions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1bc598-cf91-4d9d-9a72-7a0d6e485c7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import xarray as xr\n",
    "import os\n",
    "import sys\n",
    "import dask.distributed\n",
    "import glob\n",
    "from dask.distributed import Client\n",
    "import tempfile\n",
    "import dask\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Import utils\n",
    "sys.path.append('/home/565/dh4185/mn51-dh4185/repos_collab/nesp_bff/')\n",
    "import utils\n",
    "# Static metadata dictionaries\n",
    "from utils import locations, model_dict, cmap_dict\n",
    "\n",
    "# Import datafinder\n",
    "sys.path.append('/home/565/dh4185/mn51-dh4185/repos_collab/dataset_finder/')\n",
    "from dataset_finder import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa5baae-1b7b-4625-a131-cf95beaa692a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dask settings\n",
    "dask.config.set({\n",
    "    #'array.chunk-size': \"256 MiB\",\n",
    "    #'array.slicing.split_large_chunks': True, \n",
    "    'distributed.comm.timeouts.connect': '120s',\n",
    "    'distributed.comm.timeouts.tcp': '120s',\n",
    "    'distributed.comm.retry.count': 10,\n",
    "    'distributed.scheduler.allowed-failures': 20,\n",
    "    \"distributed.scheduler.worker-saturation\": 1.1, #< This should use the new behaviour which helps with memory pile up\n",
    "})\n",
    "\n",
    "client = Client(n_workers=25, threads_per_worker=1, local_directory = tempfile.mkdtemp(), memory_limit = \"63000mb\")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27060de5-bf9d-4450-8bc8-cc02a0c9dcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc78d38-e95c-49e1-9aad-accb99e3f8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Settings\n",
    "# Setting up the metadata for what should be computed\n",
    "# - Toggle between hourly and daily data\n",
    "# - Scenarios: historical, ssp126 or ssp370 (Note, BARRA-R2 has only historical data)\n",
    "# - RCM: BARPA-R, BARRA-R2 or CCAM-v2203-SN\n",
    "# - Start year: for reference period use 1985, for 2050 use 2035 (Note, BARRA-R2 can't take years post 2022)\n",
    "# - End year: for reference period use 2014, for 2050 use 2064   (Note, BARRA-R2 can't take years post 2022)\n",
    "# - Root directory: Computed output is saved here (don't change). Final output directory is depending on the RCM chosen\n",
    "#####\n",
    "\n",
    "# Switch between hourly (True) and daily (False) frequency\n",
    "HOURLY_FREQ = True\n",
    "_scenario = \"historical\"\n",
    "_rcm = \"CCAM-v2203-SN\"\n",
    "_gcm_ccam_1hr = \"ACCESS-CM2\"\n",
    "start_y = 1985\n",
    "end_y = 2014\n",
    "root_dir = \"/g/data/eg3/nesp_bff/step1_raw_data_extraction/\"\n",
    "\n",
    "### List of hourly and daily variables that go in the datafinder check\n",
    "vars_1hr_list = ['tas','hurs','huss','sfcWind','psl','uas','vas','clt','rsds','rsdsdir']\n",
    "vars_day_list = ['tasmax','tasmin','huss','psl','sfcWind','sfcWindmax','rsds','rsdsdir']\n",
    "\n",
    "vars_1hr = {\n",
    "    'temperature': ['tas'],\n",
    "    'cloud_cover': ['clt'],\n",
    "    'humidity_relative': ['hurs'],\n",
    "    'humidity_specific': ['huss'],\n",
    "    'wind_speed_10m': ['sfcWind'],\n",
    "    'pressure': ['psl'],\n",
    "    'wind_direction_u': ['uas'],\n",
    "    'wind_direction_v': ['vas'],\n",
    "    'cloud_cover': ['clt'],\n",
    "    'solar_global': ['rsds'],\n",
    "    'solar_direct': ['rsdsdir']\n",
    "}\n",
    "\n",
    "vars_day = {\n",
    "    'temperature_max': ['tasmax'],\n",
    "    'temperature_min': ['tasmin'],\n",
    "    'humidity_specific_max': ['huss'],\n",
    "    'humidity_specific_min': ['huss'],\n",
    "    'pressure': ['psl'],\n",
    "    'wind_speed_10m': ['sfcWind'],\n",
    "    'wind_speed_10m_max': ['sfcWindmax'],\n",
    "    'solar_global': ['rsds'],\n",
    "    'solar_direct': ['rsdsdir']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e7b118-00b0-4159-812a-6642c62bc2c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T07:02:53.493638Z",
     "iopub.status.busy": "2025-05-28T07:02:53.492876Z",
     "iopub.status.idle": "2025-05-28T07:02:53.500029Z",
     "shell.execute_reply": "2025-05-28T07:02:53.498878Z",
     "shell.execute_reply.started": "2025-05-28T07:02:53.493580Z"
    }
   },
   "source": [
    "### Check daily data availability across models\n",
    "Uses the datafinder tool from ACS to find suitable data. Handy to check if all variables, scenarios and years exist for a given RCM at **daily** timescale. More info here: https://github.com/AusClimateService/dataset_finder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7263ee0-ae2c-4ca3-9c0b-ebd56d6c5449",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#< Specify datasets - do this to find out what models have the required variables\n",
    "all_data_day = get_datasets(\"ACS_DS\",\n",
    "                        rcm = _rcm,\n",
    "                        scenario = [\"historical\",\"ssp126\",\"ssp370\"],\n",
    "                        timescale =\"day\",\n",
    "                        year = year_range(start_y, end_y))\n",
    "\n",
    "select_data_day = all_data_day.select(var = vars_day_list, exact_match=True).condense(\"scenario\")\n",
    "select_data_day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80188e1f-f29f-4798-a65b-8757cd8bc8f9",
   "metadata": {},
   "source": [
    "### Check hourly data availability across models\n",
    "Uses the datafinder tool from ACS to find suitable data. Handy to check if all variables, scenarios and years exist for a given RCM at **hourly** timescale. More info here: https://github.com/AusClimateService/dataset_finder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f65301f-70e2-4bb7-901a-0b2cb826e5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#< Specify datasets - do this to find out what models have the required variables\n",
    "all_data_1hr = get_datasets(\"ACS_DS\",\n",
    "                        rcm = _rcm,\n",
    "                        scenario = [\"historical\",\"ssp126\",\"ssp370\"],\n",
    "                        timescale = \"1hr\",\n",
    "                        year = year_range(start_y, end_y))\n",
    "\n",
    "select_data_1hr = all_data_1hr.select(var = vars_1hr_list, exact_match=True).condense(\"scenario\")\n",
    "select_data_1hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1ccb12-554f-49b2-9a23-955169870a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_day = select_data_day.find_matches(select_data_1hr, exclude_keys = \"timescale\")\n",
    "matching_1hr = select_data_1hr.find_matches(select_data_day, exclude_keys = \"timescale\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6935ee-f70c-473f-8000-16bccb2af4f7",
   "metadata": {},
   "source": [
    "## Process extraction of variables\n",
    "Does the same as the executable script __step1_extracting_variables.py__. Good for debugging or calculating individual files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8291f02-0a79-47e5-ac1f-89b42d1f8b64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Sets timescale, output directory and variable list depending on input in 'Settings' cell at the top\n",
    "freq = \"1hr\" if HOURLY_FREQ else \"day\"\n",
    "_vars = vars_1hr if HOURLY_FREQ else vars_day\n",
    "# Output location\n",
    "if _rcm == \"CCAM-v2203-SN\":\n",
    "    out_dir = f\"{root_dir}CSIRO-CCAM/\"\n",
    "else:\n",
    "    out_dir = f\"{root_dir}{_rcm}/\"\n",
    "\n",
    "print(f\"---------- {_rcm} for '{freq}' data ----------\")\n",
    "# Corrects location coordinates specified in locations dictionary in utils.py to ensure the selected grid cell from an RCM is on land.  \n",
    "updated_locations = utils.update_locations(xr.open_dataset(model_dict[_rcm][\"sftlf\"]).sftlf,locations)\n",
    "\n",
    "# ======================== MAIN LOOP ============================\n",
    "# Contains a number of print statements to track progress.\n",
    "\n",
    "# Iterating though the 12 locations in the updated locations dictionary\n",
    "for loc in updated_locations:\n",
    "    start_time_loc = time.time()  # Start timer\n",
    "    print(f\"========================== {loc} =======================\")\n",
    "    lat = updated_locations[loc]['Lat']\n",
    "    lon = updated_locations[loc]['Lon']\n",
    "    print(f\"Lat: {lat}, Lon: {lon}\")\n",
    "\n",
    "    # Iterating through GCMs for the selected RCM\n",
    "    for _gcm in model_dict[_rcm][\"gcms\"]:\n",
    "        print(f\"***** {_gcm} *****\")\n",
    "        should_continue = False\n",
    "        \n",
    "        start_time_gcm = time.time()  # Start timer\n",
    "\n",
    "        # Specifying output file name in line was naming convention\n",
    "        out_file = (\n",
    "            f\"{out_dir}{loc}_\"\n",
    "            f\"{model_dict[_rcm]['grid']}_\"\n",
    "            f\"{_gcm}_{_scenario}_\"\n",
    "            f\"{model_dict[_rcm]['gcms'][_gcm]['mdl_run']}_\"\n",
    "            f\"{model_dict[_rcm]['org']}_\"\n",
    "            f\"{_rcm}_{model_dict[_rcm]['gcms'][_gcm]['version']}_\"\n",
    "            f\"{freq}_{start_y}-{end_y}.nc\"\n",
    "        )\n",
    "\n",
    "        # Skip creation of file if it exists. Note, a file might exist in an incomplete state on disk due to an interrupted job (e.g.\n",
    "        # wall time exceeded or keyboard interrupt in the notebook) and needs to be deleted manually before executing the script again.\n",
    "        if not os.path.exists(out_file):\n",
    "            print(f\"Processing: {out_file}.....\")\n",
    "\n",
    "            # Empty var list where the extracted variables for the processed location are stored to merge them into a single dataset later\n",
    "            var_list = []\n",
    "\n",
    "            # Iterating through the variables (daily or hourly var_list)\n",
    "            for _var in _vars:\n",
    "                start_time_var = time.time()  # Start timer\n",
    "                print(f\"{_var}: {_vars[_var]}\")\n",
    "\n",
    "                # Convert boolean time specifier into string \n",
    "                _timescale = \"1hr\" if HOURLY_FREQ else \"day\"\n",
    "\n",
    "                # Maximum and minimum specific humidity (hussmax, hussmin) is not provided at daily timescale and needs to be \n",
    "                # derived from hourly data.\n",
    "                if _timescale == \"day\" and (_var == 'humidity_specific_max' or _var == 'humidity_specific_min'):\n",
    "                    print(f\"Use hourly data for {_var}.\")\n",
    "                    _timescale = \"1hr\"\n",
    "                \n",
    "                # BARPA-R is very efficiently chunked four our operation which favours little chunking across time\n",
    "                # and lots of chunking along lat and lon. CCAM is chunked for each time step but not at all\n",
    "                # along lat and lon dimensions which requires the dataset to be fully loaded. This takes con-\n",
    "                # siderable more time to process: BARPA-R day: ~2min, hourly: ~5min. CCAM daily: ~25min, hourly: >7.5h hours\n",
    "                # Hence, CCAM hourly data is preprocessed (rechunked and stored on /scratch/eg3/dh4185/) to interim files per year, \n",
    "                # only loaded if all files for one GCM are present.\n",
    "                if _rcm == \"CCAM-v2203-SN\" and _timescale == \"1hr\" and _var not in ['humidity_specific_max', 'humidity_specific_min']:# and _gcm == _gcm_ccam_1hr:\n",
    "                    print(\"Doing hourly CCAM data...\")\n",
    "                    \n",
    "                    # Read proprocessd/rechunked hourly CCAM from /scratch/eg3\n",
    "                    scratch_dir = f\"/scratch/eg3/dh4185/rechunked/{_gcm}/{_scenario}/\"\n",
    "                    rechunk_files = sorted(glob.glob(\n",
    "                        f\"{scratch_dir}{_vars[_var][0]}_\"\n",
    "                        f\"{model_dict[_rcm]['grid']}_\"\n",
    "                        f\"{_gcm}_{_scenario}_\"\n",
    "                        f\"{model_dict[_rcm]['gcms'][_gcm]['mdl_run']}_\"\n",
    "                        f\"{model_dict[_rcm]['org']}_{_rcm}_\"\n",
    "                        f\"{model_dict[_rcm]['gcms'][_gcm]['version']}_1hr_*.nc\"))\n",
    "                    \n",
    "                    if len(rechunk_files) != 30 and len(rechunk_files) >= 1:\n",
    "                        print(f\"Files don't cover 30 years from {start_y} to {end_y}. Check files and \"\n",
    "                              f\"rerun rechunk_ccam.sh\")\n",
    "                        if rechunk_files:\n",
    "                            for file in rechunk_files:\n",
    "                                print(file)\n",
    "                            should_continue = True\n",
    "                            break\n",
    "                    elif len(rechunk_files) == 0:\n",
    "                        print(f\"No files for GCM {_gcm} and {_var} exists. Run \"\n",
    "                              f\"rechunk_ccam.sh first.\")\n",
    "                        should_continue = True\n",
    "                        break\n",
    "                    else:\n",
    "                        # print(rechunk_files)\n",
    "                        # Read all years and preprocessing lat/lon selection\n",
    "                        da = xr.open_mfdataset(rechunk_files, parallel=True,\n",
    "                                                            preprocess=lambda ds: utils.preprocess_location(ds, lat, lon))[_vars[_var][0]]\n",
    "                        da = da.chunk({'time': -1}).sel(time=slice(str(start_y),str(end_y)))\n",
    "                        # Aliging time coordinates (mix of variables at half hour and full hours)\n",
    "                        da_all = utils.process_time(da,_vars[_var][0],_timescale)\n",
    "                        var_list.append(da_all.to_dataset())\n",
    "                                                \n",
    "                        print(f\"Processing time for {_var}: {((time.time() - start_time_var)/60):.2f} minutes\\n\")\n",
    "\n",
    "\n",
    "                # If BARPA-R or BARRA-R2 at daily or hourly timescale, or CCAM at daily time scale selected\n",
    "                # process all years at once.\n",
    "                elif _rcm in [\"BARPA-R\",\"BARRA-R2\"] or _rcm == \"CCAM-v2203-SN\" and _timescale == \"day\":\n",
    "\n",
    "                    # Get file paths using the ACS dataset finder\n",
    "                    in_dir = model_dict[_rcm][\"root_dir\"]\n",
    "                    all_data = get_datasets(\"ACS_DS\",\n",
    "                                rcm = _rcm,\n",
    "                                gcm = _gcm,\n",
    "                                scenario = _scenario,\n",
    "                                grid = model_dict[_rcm][\"grid\"],\n",
    "                                org = model_dict[_rcm][\"org\"],\n",
    "                                mdl_run = model_dict[_rcm][\"gcms\"][_gcm][\"mdl_run\"],\n",
    "                                ver = model_dict[_rcm][\"gcms\"][_gcm][\"version\"],\n",
    "                                timescale = _timescale,\n",
    "                                year = year_range(start_y, end_y)).select(var = _vars[_var], exact_match=True)\n",
    "\n",
    "                    # Read all years and preprocessing lat/lon selection\n",
    "                    da = xr.open_mfdataset(all_data.get_files(), parallel=True,\n",
    "                                            preprocess=lambda ds: utils.preprocess_location(ds, lat, lon))[_vars[_var][0]]\n",
    "                    da = da.chunk({'time': -1})\n",
    "                    # Using hourly huss data to determine daily hussmax and hussmin\n",
    "                    da_temp = utils.process_humidity(da,_var)\n",
    "                    # Aliging time coordinates (mix of variables at half hour and full hours)\n",
    "                    da_all = utils.process_time(da_temp,_vars[_var][0],_timescale)\n",
    "                    var_list.append(da_all.to_dataset())\n",
    "\n",
    "                else:\n",
    "                    print(\"Inappropriate RCM, GCM, timescale requested. Check Settings.\")\n",
    "                    break\n",
    "                                \n",
    "                print(f\"Processing time for {_var}: {((time.time() - start_time_var)/60):.2f} minutes\\n\")\n",
    "\n",
    "            if should_continue:\n",
    "                print(\"Move to the next GCM.\")\n",
    "                continue  # Move to the next GCM\n",
    "                \n",
    "            # Remove unwanted variables\n",
    "            cleaned_list = [da.drop_vars([\"bnds\",\"height\",\"level_height\",\"model_level_number\",\"sigma\"], errors=\"ignore\") for da in var_list]\n",
    "\n",
    "            # Merge all variables per GCM\n",
    "            da_var = xr.merge(cleaned_list)\n",
    "            print(da_var)\n",
    "            # Write to disk\n",
    "            da_var.to_netcdf(out_file)\n",
    "\n",
    "            print(f\"Processing time for {_rcm}-{_gcm}: {((time.time() - start_time_gcm)/60):.2f} minutes\\n\")\n",
    "\n",
    "        else:\n",
    "            print(f'File for {loc} exists in output directory.')\n",
    "\n",
    "    print(f\"Processing time for {loc}: {((time.time() - start_time_loc)/60):.2f} minutes\\n\")\n",
    "    \n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc88640-8f3c-4a58-8e2a-130d6877d75d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#####################################################\n",
    "################ For hourly CCAM data ###############\n",
    "#####################################################\n",
    "\n",
    "# Sets timescale, output directory and variable list depending on input in 'Settings' cell at the top\n",
    "freq = \"1hr\" if HOURLY_FREQ else \"day\"\n",
    "_vars = vars_1hr if HOURLY_FREQ else vars_day\n",
    "_gcm = \"ACCESS-CM2\"\n",
    "\n",
    "# Output location\n",
    "if _rcm == \"CCAM-v2203-SN\":\n",
    "    out_dir = f\"{root_dir}CSIRO-CCAM/\"\n",
    "else:\n",
    "    out_dir = f\"{root_dir}{_rcm}/\"\n",
    "\n",
    "print(f\"---------- {_rcm} for '{freq}' data ----------\")\n",
    "# Corrects location coordinates specified in locations dictionary in utils.py to ensure the selected grid cell from an RCM is on land.  \n",
    "updated_locations = utils.update_locations(xr.open_dataset(model_dict[_rcm][\"sftlf\"]).sftlf,locations)\n",
    "\n",
    "# ======================== MAIN LOOP ============================\n",
    "# Contains a number of print statements to track progress.\n",
    "\n",
    "# Iterating though the 12 locations in the updated locations dictionary\n",
    "for loc in updated_locations:\n",
    "    start_time_loc = time.time()  # Start timer\n",
    "    print(f\"========================== {loc} =======================\")\n",
    "    lat = updated_locations[loc]['Lat']\n",
    "    lon = updated_locations[loc]['Lon']\n",
    "    print(f\"Lat: {lat}, Lon: {lon}\")\n",
    "    print(f\"***** {_gcm} *****\")\n",
    "        \n",
    "    start_time_gcm = time.time()  # Start timer\n",
    "\n",
    "    # Specifying output file name in line was naming convention\n",
    "    out_file = (\n",
    "        f\"{out_dir}{loc}_\"\n",
    "        f\"{model_dict[_rcm]['grid']}_\"\n",
    "        f\"{_gcm}_{_scenario}_\"\n",
    "        f\"{model_dict[_rcm]['org']}_\"\n",
    "        f\"{model_dict[_rcm]['gcms'][_gcm]['mdl_run']}_\"\n",
    "        f\"{_rcm}_{model_dict[_rcm]['gcms'][_gcm]['version']}_\"\n",
    "        f\"{freq}_{start_y}-{end_y}.nc\"\n",
    "    )\n",
    "\n",
    "    # Skip creation of file if it exists. Note, a file might exist in an incomplete state on disk due to an interrupted job (e.g.\n",
    "    # wall time exceeded or keyboard interrupt in the notebook) and needs to be deleted manually before executing the script again.\n",
    "    if not os.path.exists(out_file):\n",
    "        print(f\"Processing: {out_file}.....\")\n",
    "\n",
    "        # Empty var list where the extracted variables for the processed location are stored to merge them into a single dataset later\n",
    "        var_list = []\n",
    "\n",
    "        # Iterating through the variables (daily or hourly var_list)\n",
    "        for _var in _vars:\n",
    "            start_time_var = time.time()  # Start timer\n",
    "            print(f\"{_var}: {_vars[_var]}\")\n",
    "\n",
    "            # Convert boolean time specifier into string \n",
    "            _timescale = \"1hr\" if HOURLY_FREQ else \"day\"\n",
    "\n",
    "            # Read proprocessd/rechunked hourly CCAM from /scratch/eg3\n",
    "            scratch_dir = f\"/scratch/eg3/dh4185/rechunked/{_gcm}/{_scenario}/\"\n",
    "            rechunk_files = sorted(glob.glob(\n",
    "                f\"{scratch_dir}{_vars[_var][0]}_\"\n",
    "                f\"{model_dict[_rcm]['grid']}_\"\n",
    "                f\"{_gcm}_{_scenario}_\"\n",
    "                f\"{model_dict[_rcm]['gcms'][_gcm]['mdl_run']}_\"\n",
    "                f\"{model_dict[_rcm]['org']}_{_rcm}_\"\n",
    "                f\"{model_dict[_rcm]['gcms'][_gcm]['version']}_1hr_*.nc\"))\n",
    "            \n",
    "            if len(rechunk_files) != 30:\n",
    "                print(f\"Files don't cover 30 years from {start_y} to {end_y}. Check files and \"\n",
    "                      f\"rerun rechunk_ccam.sh\")\n",
    "                if rechunk_files:\n",
    "                    for file in rechunk_files:\n",
    "                        print(file)\n",
    "            else:\n",
    "                # print(rechunk_files)\n",
    "                # Read all years and preprocessing lat/lon selection\n",
    "                da = xr.open_mfdataset(rechunk_files, parallel=True,\n",
    "                                                preprocess=lambda ds: utils.preprocess_location(ds, lat, lon))[_vars[_var][0]]\n",
    "                da = da.chunk({'time': -1}).sel(time=slice(str(start_y),str(end_y)))\n",
    "                # Aliging time coordinates (mix of variables at half hour and full hours)\n",
    "                da_all = utils.process_time(da,_var,_timescale)\n",
    "                var_list.append(da_all.to_dataset())\n",
    "                                    \n",
    "                print(f\"Processing time for {_var}: {((time.time() - start_time_var)/60):.2f} minutes\\n\")\n",
    "\n",
    "        # Remove unwanted variables\n",
    "        cleaned_list = [da.drop_vars([\"bnds\",\"height\",\"level_height\",\"model_level_number\",\"sigma\"], errors=\"ignore\") for da in var_list]\n",
    "\n",
    "        # Merge all variables per GCM\n",
    "        da_var = xr.merge(cleaned_list)\n",
    "        print(da_var)\n",
    "        # Write to disk\n",
    "        da_var.to_netcdf(out_file)\n",
    "\n",
    "        print(f\"Processing time for {_rcm}-{_gcm}: {((time.time() - start_time_gcm)/60):.2f} minutes\\n\")\n",
    "\n",
    "    else:\n",
    "        print(f'File for {loc} exists in output directory.')\n",
    "\n",
    "    print(f\"Processing time for {loc}: {((time.time() - start_time_loc)/60):.2f} minutes\\n\")\n",
    "    \n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31b5d6d-47e0-4ba2-8656-3cab785a6d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rechunk_dir = f\"/scratch/eg3/dh4185/rechunked/ACCESS-CM2/{_scenario}/\"\n",
    "lst = []\n",
    "for Var in vars_1hr_list:\n",
    "    print(Var)\n",
    "    rechunk_files = sorted(glob.glob(f\"{rechunk_dir}{Var}_AUS-10i_ACCESS-CM2_{_scenario}_r4i1p1f1_CSIRO_CCAM-v2203-SN_v1-r1_1hr_*.nc\"))\n",
    "    # print(len(rechunk_files))\n",
    "    # Read all years and preprocessing lat/lon selection\n",
    "    da = xr.open_mfdataset(rechunk_files, parallel=True,\n",
    "                            preprocess=lambda ds: utils.preprocess_location(ds, -15, 140))[Var]\n",
    "    da = da.chunk({'time': -1})\n",
    "    # print(da)\n",
    "    da_time = utils.process_time(da,Var,\"1hr\")\n",
    "    print(da_time)\n",
    "\n",
    "    lst.append(da_time.to_dataset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2771335f-51f2-4c46-8015-8a12cc3d47c4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Old script with processing CCAM hourly data one file at a time\n",
    "%%time\n",
    "#####################################################\n",
    "######## For everything NOT hourly CCAM data ########\n",
    "#####################################################\n",
    "\n",
    "# Sets timescale, output directory and variable list depending on input in 'Settings' cell at the top\n",
    "freq = \"1hr\" if HOURLY_FREQ else \"day\"\n",
    "_vars = vars_1hr if HOURLY_FREQ else vars_day\n",
    "# Output location\n",
    "if _rcm == \"CCAM-v2203-SN\":\n",
    "    out_dir = f\"{root_dir}CSIRO-CCAM/\"\n",
    "else:\n",
    "    out_dir = f\"{root_dir}{_rcm}/\"\n",
    "\n",
    "print(f\"---------- {_rcm} for '{freq}' data ----------\")\n",
    "# Corrects location coordinates specified in locations dictionary in utils.py to ensure the selected grid cell from an RCM is on land.  \n",
    "updated_locations = utils.update_locations(xr.open_dataset(model_dict[_rcm][\"sftlf\"]).sftlf,locations)\n",
    "\n",
    "# ======================== MAIN LOOP ============================\n",
    "# Contains a number of print statements to track progress.\n",
    "\n",
    "# Iterating though the 12 locations in the updated locations dictionary\n",
    "for loc in updated_locations:\n",
    "    start_time_loc = time.time()  # Start timer\n",
    "    print(f\"========================== {loc} =======================\")\n",
    "    lat = updated_locations[loc]['Lat']\n",
    "    lon = updated_locations[loc]['Lon']\n",
    "    print(f\"Lat: {lat}, Lon: {lon}\")\n",
    "\n",
    "    # Iterating through GCMs for the selected RCM\n",
    "    for _gcm in model_dict[_rcm][\"gcms\"]:\n",
    "        print(f\"***** {_gcm} *****\")\n",
    "        \n",
    "        # File list to story temporary files if CCAM and hourly time scale are selected\n",
    "        files_gcm_list = []\n",
    "        start_time_gcm = time.time()  # Start timer\n",
    "\n",
    "        # Specifying output file name in line was naming convention\n",
    "        out_file = (\n",
    "            f\"{out_dir}{loc}_\"\n",
    "            f\"{model_dict[_rcm]['grid']}_\"\n",
    "            f\"{_gcm}_{_scenario}_\"\n",
    "            f\"{model_dict[_rcm]['gcms'][_gcm]['mdl_run']}_\"\n",
    "            f\"{model_dict[_rcm]['org']}_\"\n",
    "            f\"{_rcm}_{model_dict[_rcm]['gcms'][_gcm]['version']}_\"\n",
    "            f\"{freq}_{start_y}-{end_y}.nc\"\n",
    "        )\n",
    "\n",
    "        # Skip creation of file if it exists. Note, a file might exist in an incomplete state on disk due to an interrupted job (e.g.\n",
    "        # wall time exceeded or keyboard interrupt in the notebook) and needs to be deleted manually before executing the script again.\n",
    "        if not os.path.exists(out_file):\n",
    "            print(f\"Processing: {out_file}.....\")\n",
    "\n",
    "            # Empty var list where the extracted variables for the processed location are stored to merge them into a single dataset later\n",
    "            var_list = []\n",
    "\n",
    "            # Iterating through the variables (daily or hourly var_list)\n",
    "            for _var in _vars:\n",
    "                start_time_var = time.time()  # Start timer\n",
    "                print(f\"{_var}: {_vars[_var]}\")\n",
    "\n",
    "                # Convert boolean time specifier into string \n",
    "                _timescale = \"1hr\" if HOURLY_FREQ else \"day\"\n",
    "\n",
    "                # Maximum and minimum specific humidity (hussmax, hussmin) is not provided at daily timescale and needs to be \n",
    "                # derived from hourly data.\n",
    "                if _timescale == \"day\" and (_var == 'humidity_specific_max' or _var == 'humidity_specific_min'):\n",
    "                    print(f\"Use hourly data for {_var}.\")\n",
    "                    _timescale = \"1hr\"\n",
    "\n",
    "                # Get file paths using the ACS dataset finder\n",
    "                in_dir = model_dict[_rcm][\"root_dir\"]\n",
    "                all_data = get_datasets(\"ACS_DS\",\n",
    "                                rcm = _rcm,\n",
    "                                gcm = _gcm,\n",
    "                                scenario = _scenario,\n",
    "                                grid = model_dict[_rcm][\"grid\"],\n",
    "                                org = model_dict[_rcm][\"org\"],\n",
    "                                mdl_run = model_dict[_rcm][\"gcms\"][_gcm][\"mdl_run\"],\n",
    "                                ver = model_dict[_rcm][\"gcms\"][_gcm][\"version\"],\n",
    "                                timescale = _timescale,\n",
    "                                year = year_range(start_y, end_y)).select(var = _vars[_var], exact_match=True)\n",
    "                \n",
    "                # BARPA-R is very efficiently chunked four our operation which favours little chunking across time\n",
    "                # and lots of chunking along lat and lon. CCAM is chunked for each time step but not at all\n",
    "                # along lat and lon dimensions which requires the dataset to be fully loaded. This takes con-\n",
    "                # siderable more time to process: BARPA-R day: ~2min, hourly: ~5min. CCAM daily: ~25min, hourly: >7.5h hours\n",
    "                # Hence, CCAM hourly data is preprocessed to interim files per year, and then loaded and concatenated.\n",
    "                if _rcm == \"CCAM-v2203-SN\" and _timescale == \"1hr\" and _var not in ['humidity_specific_max', 'humidity_specific_min']:\n",
    "                    print(\"Creating temporary files for variables.\")\n",
    "                    start_time_CCAM_1hr = time.time()\n",
    "                    files = sorted(all_data.get_files())\n",
    "                    temp_dir = \"/g/data/eg3/nesp_bff/step1_raw_data_extraction/CSIRO-CCAM/temp/\"\n",
    "\n",
    "                    pattern = f\"{temp_dir}{loc}_{files[0].split('/')[-1][:-20]}*.nc\"\n",
    "                    matching_files = sorted(glob.glob(pattern))\n",
    "                            \n",
    "                    if matching_files:\n",
    "                        # Sort by modification time (latest last)\n",
    "                        most_recent_file = max(matching_files, key=os.path.getmtime)\n",
    "                        print(f\"Deleting most recent temp file for safety: {most_recent_file}\")\n",
    "                        try:\n",
    "                            os.remove(most_recent_file)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Could not delete {most_recent_file}: {e}\")\n",
    "    \n",
    "                    file_list = []\n",
    "                    for file in files:\n",
    "                        print(\"Doing time period: \",file.split(\"_\")[-1][:-3])\n",
    "                        out_file_temp = f\"{temp_dir}{loc}_{file.split('/')[-1]}\"\n",
    "\n",
    "                        # Validate if the file exists and is readable. If not, delete them and \n",
    "                        # recompute. Needed if previous job didn't finish because it hit the walltime and \n",
    "                        # an unfinished file is sitting in the directory.\n",
    "                        # Check if file exists\n",
    "                        if os.path.exists(out_file_temp):\n",
    "                            try:\n",
    "                                # Try to open the file to verify it's valid\n",
    "                                test = xr.open_dataset(out_file_temp, engine=\"netcdf4\")\n",
    "                                test.close()\n",
    "                                print(f\"Valid temp file already exists: {out_file_temp}\")\n",
    "                            except (OSError, KeyError, ValueError, RuntimeError) as e:\n",
    "                                print(f\"Warning: {out_file_temp} exists but could not be opened (possibly corrupted). Deleting and recreating.\")\n",
    "                                os.remove(out_file_temp)\n",
    "                            \n",
    "                        # Create file if it doesn't already exist\n",
    "                        if not os.path.exists(out_file_temp):\n",
    "                            da = xr.open_dataset(file, chunks={'time':-1}).sel(lat=lat,\n",
    "                                                                                   lon=lon,\n",
    "                                                                                   method=\"nearest\")[_vars[_var][0]]\n",
    "                            # Aliging time coordinates (mix of variables at half hour and full hours)\n",
    "                            da_temp = utils.process_time(da,_vars[_var],_timescale)\n",
    "                            ds = da_temp.to_dataset()\n",
    "\n",
    "                            # Write temporary file for var and year\n",
    "                            ds.to_netcdf(out_file_temp)\n",
    "                            if hasattr(da, 'close'):\n",
    "                                da.close()\n",
    "\n",
    "                        # Collecting yearly file names per variable\n",
    "                        file_list.append(out_file_temp)\n",
    "                        # Collecting all files names for a GCM (all variables) to collectively delete them from\n",
    "                        # temp directory when final file is written\n",
    "                        files_gcm_list.append(out_file_temp)\n",
    "                        \n",
    "                    # Open all years for each variable and concatenating them along time\n",
    "                    da_all = xr.open_mfdataset(file_list, combine='nested',\n",
    "                                                concat_dim='time', parallel=True)\n",
    "                    print(da_all)\n",
    "                    # Append variable dataset list\n",
    "                    var_list.append(da_all)\n",
    "                    da_all.close()\n",
    "                    print(f\"Processing time for {_rcm}-{_gcm} {_var}: {((time.time() - start_time_CCAM_1hr)/60):.2f} minutes\\n\")\n",
    "\n",
    "                # If BARPA-R or BARRA-R2 at daily or hourly timescale, or CCAM at daily time scale selected\n",
    "                # process all years at once.\n",
    "                else:\n",
    "                    # Read all years and preprocessing lat/lon selection\n",
    "                    da = xr.open_mfdataset(all_data.get_files(), parallel=True,\n",
    "                                            preprocess=lambda ds: utils.preprocess_location(ds, lat, lon))[_vars[_var][0]]\n",
    "                    da = da.chunk({'time': -1})\n",
    "                    # Using hourly huss data to determine daily hussmax and hussmin\n",
    "                    da_temp = utils.process_humidity(da,_var)\n",
    "                    # Aliging time coordinates (mix of variables at half hour and full hours)\n",
    "                    da_all = utils.process_time(da_temp,_var,_timescale)\n",
    "                    var_list.append(da_all.to_dataset())\n",
    "                                \n",
    "                print(f\"Processing time for {_var}: {((time.time() - start_time_var)/60):.2f} minutes\\n\")\n",
    "\n",
    "            # Remove unwanted variables\n",
    "            cleaned_list = [da.drop_vars([\"bnds\",\"height\",\"level_height\",\"model_level_number\",\"sigma\"], errors=\"ignore\") for da in var_list]\n",
    "\n",
    "            # Merge all variables per GCM\n",
    "            da_var = xr.merge(cleaned_list)\n",
    "            print(da_var)\n",
    "            # Write to disk\n",
    "            da_var.to_netcdf(out_file)\n",
    "\n",
    "            # Remove temporary files from disk\n",
    "            for f in files_gcm_list:\n",
    "                os.remove(f)\n",
    "            print(f\"Processing time for {_rcm}-{_gcm}: {((time.time() - start_time_gcm)/60):.2f} minutes\\n\")\n",
    "\n",
    "        else:\n",
    "            print(f'File for {loc} exists in output directory.')\n",
    "\n",
    "    print(f\"Processing time for {loc}: {((time.time() - start_time_loc)/60):.2f} minutes\\n\")\n",
    "    \n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:analysis3]",
   "language": "python",
   "name": "conda-env-analysis3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
